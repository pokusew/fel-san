{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SAN Assignment - Linear discriminant analysis\n",
    "\n",
    "Author : Your Name \\\n",
    "Email  : you@fel.cvut.cz"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "93f2407c",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "The aim of this assignment is to get familiar with Linear Discriminant Analysis\n",
    "(LDA). LDA and Principal Component Analysis (PCA) are two techniques\n",
    "for dimensionality reduction. PCA can be described as an unsupervised algorithm that ignores data labels and aims to find directions which maximalize\n",
    "the variance in a data. In comparison with PCA, LDA is a supervised algorithm and aims to project a dataset onto a lower dimensional space with good\n",
    "class separability. In other words, LDA maximalizes the ratio of betweenclass variance and the within-class variance in a given data."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Input data \n",
    "In this tutorial, we will work with a dataset that classifies wines (samples)\n",
    "into three classes using of 13 continuous attributes; for more details see\n",
    "wine info.txt file. The dataset is located at wine.csv.\n",
    "\n",
    "#### Linear Discriminant Analysis\n",
    "As we mentioned above, LDA finds directions where classes are well-separated,\n",
    "i.e. LDA maximizes the ratio of between-class variance and the within-class\n",
    "variance. Firstly, assume that $C$ is a set of classes and set $D$, which represents\n",
    "a training dataset, is defined as $D = \\{x_1, x_2, . . . , x_N \\}$.\n",
    "\n",
    "The between-classes scatter matrix SB is defined as:\n",
    "$S_b = \\sum_c N_C(\\mu_c -\\overline{x})(\\mu_c - \\overline{x})^T$, where $\\overline{x}$ is a vector represents the overall mean of the data, Âµ represents the mean corresponding to each class, and $N_C$ are sizes of the respective classes.\n",
    "\n",
    "The within-classes scatter matrix $S_W$ is defined as:\n",
    "\n",
    "$S_W = \\sum_c \\sum_{x \\in D_c}(x - \\overline{\\mu_c})(x - \\overline{\\mu_c})^T$\n",
    "\n",
    "Next, we will solve the generalized eigenvalue problem for the matrix $S_W^{-1}S_B$ to obtain the linear discriminants, i.e.\n",
    "\n",
    "$(S_W^{-1}S_B)w = \\lambda w$\n",
    "\n",
    "where $w$ represents an eigenvector and $\\lambda$ represents an eigenvalue. Finally,\n",
    "choose k eigenvectors with the largest eigenvalue and transform the samples\n",
    "onto the new subspace.\n",
    "\n",
    "\n",
    "##### imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step by step\n",
    "\n",
    "##### Load the dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "15c8f8b3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "##### Compute the within-scatter matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_within_scatter(data):\n",
    "    \"\"\" Function to compute within scatter matrix S_w.\n",
    "\n",
    "    :param data: list of numpy arrays with data from individual classes\n",
    "    \"\"\"\n",
    "    # todo: replace with your code\n",
    "    within_matrix = np.eye(data[0].shape[1])\n",
    "\n",
    "    return within_matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "1caefb32",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "##### Compute the between-scatter matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_between_scatter(data):\n",
    "    \"\"\" Function to compute between scatter matrix S_b.\n",
    "\n",
    "    :param data: list of numpy arrays with data from individual classes\n",
    "    \"\"\"\n",
    "    # todo: replace with your code\n",
    "    between_matrix = np.eye(data[0].shape[1])\n",
    "\n",
    "    return between_matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "c784202b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "##### Solve the EigenProblem and return eigen-vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def solve_eigen_problem(within_matrix, between_matrix):\n",
    "    \"\"\" Compute eigenvectors as defined by the method.\n",
    "\n",
    "    Carefully with the datatypes here - numpy will give you complex numbers.\n",
    "    This does not affect the functionallity, but it does give you a ton of warnings.\n",
    "\n",
    "    :param within_matrix: numpy array (n_variables, n_variables)\n",
    "    :param between_matrix: numpy array, same shape\n",
    "    :return: real eigenvectors, ordered from the highest eigenvalue, numpy array, same shape\n",
    "    \"\"\"\n",
    "\n",
    "    # todo: replace with your code\n",
    "    eigenvectors = None\n",
    "\n",
    "    return eigenvectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "6841d4b3",
   "metadata": {},
   "source": [
    "##### Visualize the results\n",
    "Project your data into lower-dimensional subspace, visualize this projection, and compare with PCA (see Fig. 1). Use the following code while filling in the lines marked as `TODO`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def split_by_class(data, labels):\n",
    "    ret = []\n",
    "    for lbl in np.unique(labels):\n",
    "        ret.append(data[labels == lbl])\n",
    "    return ret\n",
    "\n",
    "\n",
    "def compute_centroids(data, labels):\n",
    "    ret = []\n",
    "    for lbl in np.unique(labels):\n",
    "        ret.append(data[labels == lbl].mean(axis=0))\n",
    "    return np.array(ret)\n",
    "\n",
    "\n",
    "def classify(data, eigen_vectors, centroids, prior):\n",
    "    n_classes = len(centroids)\n",
    "\n",
    "    y = data @ eigen_vectors[:, :n_classes]\n",
    "    classif = np.empty((data.shape[0], n_classes))\n",
    "    for c in range(n_classes):\n",
    "        classif[:, c] = ((y - centroids[c]) ** 2 / 2).sum(axis=1) - np.log(prior[c])\n",
    "    return classif.argmin(axis=1) + 1  # indexes start 0, labels start as 1\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def LDA(data, labels):\n",
    "    data_per_class = split_by_class(data, labels)\n",
    "\n",
    "    # 1) scatter matrices\n",
    "    # within-class scatter matrix Sw\n",
    "    S_w = compute_within_scatter(data_per_class)\n",
    "\n",
    "    # between-class scatter matrix Sb\n",
    "    S_b = compute_between_scatter(data_per_class)\n",
    "\n",
    "    # 2)  eigen problem\n",
    "    # solve eigen problem\n",
    "    eigenvectors = solve_eigen_problem(S_w, S_b)\n",
    "\n",
    "    #transform the samples onto the new subspace\n",
    "    y = data @ eigenvectors[:, :2]\n",
    "\n",
    "    ## visual comparison with PCA\n",
    "    scaled_data = StandardScaler().fit_transform(data)\n",
    "    y_pca = PCA(2).fit_transform(data)\n",
    "    y_pca_scaled = PCA(2).fit_transform(scaled_data)\n",
    "\n",
    "    fig, axs = plt.subplots(ncols=3)\n",
    "    axs[0].scatter(y[:, 0], y[:, 1], c=labels)\n",
    "    axs[0].set_xlabel(\"LDA_1\")\n",
    "    axs[0].set_ylabel(\"LDA_2\")\n",
    "    axs[0].set_title(\"LDA\")\n",
    "\n",
    "    axs[1].scatter(y_pca[:, 0], y_pca[:, 1], c=labels)\n",
    "    axs[1].set_xlabel(\"PCA_1\")\n",
    "    axs[1].set_ylabel(\"PCA_2\")\n",
    "    axs[1].set_title(\"PCA not scaled\")\n",
    "\n",
    "    axs[2].scatter(y_pca_scaled[:, 0], y_pca_scaled[:, 1], c=labels)\n",
    "    axs[2].set_xlabel(\"PCA_1\")\n",
    "    axs[2].set_ylabel(\"PCA_2\")\n",
    "    axs[2].set_title(\"PCA scaled\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "\n",
    "    return eigenvectors\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def crossvalidation_lda(data, labels, n_folds=10):\n",
    "    seed = 0\n",
    "    n_classes = len(np.unique(labels))\n",
    "    accuracy = []\n",
    "    skf = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "    for train_idxs, test_idxs in skf.split(data, labels):\n",
    "        eigen_lda = LDA(data[train_idxs], labels[train_idxs])\n",
    "        projected_data = data[train_idxs] @ eigen_lda[:, :n_classes]\n",
    "        centroids = compute_centroids(projected_data, labels[train_idxs])\n",
    "        predicted_labels = classify(data[test_idxs], eigen_lda, centroids,\n",
    "                                    np.full(n_classes, 1. / n_classes))\n",
    "        accuracy.append(np.mean(predicted_labels == labels[test_idxs]))\n",
    "    return np.mean(accuracy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "############################# MAIN ##########################################\n",
    "\n",
    "df = pd.read_csv(\"wine.csv\", header=None)\n",
    "labels = df.iloc[:, 0].values\n",
    "data = df.iloc[:, 1:].values\n",
    "\n",
    "n_classes = len(np.unique(labels))\n",
    "\n",
    "#compute LDA and return corresponding eigenvectors\n",
    "eigen_lda = LDA(data, labels)\n",
    "\n",
    "projected_data = data @ eigen_lda[:, :n_classes]\n",
    "centroids = compute_centroids(projected_data, labels)\n",
    "\n",
    "predicted_labels = classify(data, eigen_lda, centroids, np.full(n_classes, 1. / n_classes))\n",
    "\n",
    "#ACC\n",
    "print(\"accuracy:\", np.mean(predicted_labels == labels))\n",
    "\n",
    "#CrossValidation\n",
    "acc_lda = crossvalidation_lda(data, labels, n_folds=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "9ba6ff71",
   "metadata": {},
   "source": [
    "##### Discuss given results and copare the methods.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "Python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

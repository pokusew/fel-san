---
title: "PCA in a real dataset"
output: pdf_document
---

```{r setup, include=FALSE, warning=FALSE}
library(dplyr)
# install.packages("factoextra")
library(factoextra)
```

## PCA in a medical dataset
Breast cancer dataset can be also downloaded here https://www.kaggle.com/uciml/breast-cancer-wisconsin-data

```{r}
data <- read.csv("breast_cancer.csv")
data <- data %>%
	dplyr::select(-id, -X) # remove unnecessary columns

# Explore the data (e.g. with glimpse()). How many dimensions are there?
# -> 30 features
data_numeric <- data %>%
	dplyr::select(-diagnosis)

# Perform PCA
data_pca <- prcomp(data_numeric)

lbs <- data$diagnosis # Save the diagnosis as color labels

# Plot the PCA projection. Is it a good projection given how many features we have?
# -> 30 features
fviz_pca_ind(
	data_pca, geom.ind = "point", pointshape = 21,
	fill.ind = lbs,
	palette = "jco",
	addEllipses = TRUE,
	legend.title = "Diagnosis"
) +
	ggtitle("Breast cancer 2D PCA-plot") +
	theme(plot.title = element_text(hjust = 0.5))

# Questions>
# 1. What are the disadvantages using all features e.g. in a classification task and visualization?
# 2. How would you choose the "right ones"?

# reducing dimensionality
# - feature selection: LASSO
# - LDA
# - dimensionality reducation: PCA (intridcues new axes, gets rid of the old ones)
```

## tSNE approach to dimensionality reduction

```{r}
# install.packages("Rtsne")
library(Rtsne)

# Learn about the perplexity argument in the docs and try different values (suggested values are between 10 and 100)
# Perplexity parameter should not be bigger than 3 * perplexity < nrow(X) - 1, see details for interpretation.
tsne_res <- Rtsne::Rtsne(data_numeric, perplexity = 60)

# Plot the t-SNE projection
tsne_res$Y %>%
	as.data.frame() %>%
	# Add labels to the projection
	mutate(diagnosis = data$diagnosis) %>%
	ggplot(aes(x = V1, y = V2, col = diagnosis)) +
	geom_point()

# Questions>
# What happens with the projection if you increase/decrease the perplexity values?
#   smaller perplexity > greater resolution, it can capture more topology
# How does the projection differ compared to PCA?
#   PCA uses linear sth.
```

## PCA revisited
Test your understanding of PCA after learning its mechanics.
```{r}
# How many components do we need to sufficiently capture data?
summary(data_pca)

# What is the primary composition of first two principal components
biplot(data_pca)

# The labels in the plot might be too dense. Can you obtain the same information other way?
____
```



```{r}
# LDA
labels <- data$diagnosis %>% as.factor()
# Load the LDA() and other supporting function from the 3rd assignment into the environment.
# Now you can compare LDA and PCA dimensionality reduction:
eigenLDA <- LDA(data_numeric, labels)
```

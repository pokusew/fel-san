#' Gaussian mixture model (GMM) with Expectation Maximization (EM) algorithm.
#'
#' Trains GMM model using EM algorithm.
#' The trained model then an be used with [gmm_em_train()].
#'
#' @param training_data matrix of type (num_training_points x dim) with the training data
#' @param num_components number of Gaussian components, a positive scalar integer, (hyperparameter)
#' @param max_num_steps EM algorithm max number of steps (non-zero integer),
#'        when `NULL` is given the number of steps is unlimited
#' @param stop_diff EM algorithm max difference in parameters for convergence
#' @param .debug `TRUE` enables debug prints, `FALSE` (default) disables them
#'
#' @seealso https://people.csail.mit.edu/rameshvs/content/gmm-em.pdf
#' @seealso https://en.wikipedia.org/wiki/Mixture_model#Multivariate_Gaussian_mixture_model
#' @seealso https://brilliant.org/wiki/gaussian-mixture-model/
#'
#' @return the trained model, a list with the following named attributes:
#'           * dim - the dimension of the training data
#'           * num_components - number of Gaussian components, a positive scalar integer, (hyperparameter)
#'           * alphas - trained weights of the Gaussian components, vector of size num_components
#'           * means - trained means of the Gaussian components, matrix of type (num_components x dim)
#'           * cov_matrices - trained convariance matrices of the Gaussian components,
#'                            matrix (num_components x 1) of convariance matrices (dim x dim)
#'           * em_num_steps - the number of performed EM algorithm iterations (steps),
#'                            em_num_steps <= max_num_steps (of not `NULL`
#'           * em_covergence - TRUE when convergence was reached, FALSE otherwise
#'
gmm_em_train <- function(training_data, num_components, max_num_steps = 200, stop_diff = 1e-5, .debug = FALSE) {

  if (
    !is.numeric(num_components) ||
      length(num_components) != 1 ||
      num_components < 1
  ) {
    stop("invalid value for num_components argument given")
  }

  num_training_points <- nrow(training_data)
  d <- ncol(training_data)

  # initilization step
  c_alphas <- rep(1 / num_components, times = num_components)
  c_means <- training_data[sample(seq_len(num_training_points), size = num_components, replace = FALSE),]
  c_cov_matrices <- matrix(list(cov(training_data)), nrow = num_components, ncol = 1)
  c_cov_matrices <- saturate_cov_matrices(c_cov_matrices, .debug = .debug)

  # likelihood_matrix[t, k] = the probability that x_t is generated by component C_k
  likelihood_matrix <- matrix(0, nrow = num_training_points, ncol = num_components)

  convergence <- FALSE
  step <- 1

  while (TRUE) {

    if (is.numeric(max_num_steps) && step > max_num_steps) {
      cat(paste0("[gmm_em_train] EM max_num_steps=", max_num_steps, " reached while convergence NOT reached\n"))
      step <- step - 1  # decrement so the the value in the returned model list is correct
      break
    }

    if (.debug) {
      cat(paste0("[gmm_em_train] EM step=", step, "\n"))
    }

    # expectation (E) step
    for (k in seq_len(num_components)) {
      p_num <- c_alphas[k] * gmm(x = training_data, mean = c_means[k,], cov_matrix = c_cov_matrices[[k]])
      p_denom <- rep(0, times = num_training_points)
      for (j in seq_len(num_components)) {
        pp <- c_alphas[j] * gmm(x = training_data, mean = c_means[j,], cov_matrix = c_cov_matrices[[j]])
        p_denom <- p_denom + pp
      }
      p <- p_num / p_denom
      likelihood_matrix[, k] <- p
    }

    # maximization (M) step
    # note: the order of comuptation is important here
    # 1. alphas
    next_c_alphas <- colMeans(likelihood_matrix)
    # 2. means
    # note: the following for could be probably vectorized
    next_c_means <- matrix(0, nrow = num_components, ncol = d)
    for (k in seq_len(num_components)) {
      next_c_means[k,] <- colSums(sweep(training_data, MARGIN = 1, STATS = likelihood_matrix[, k], FUN = `*`)) / sum(likelihood_matrix[, k])
    }
    # 3. covariance matrices
    next_c_cov_matrices <- matrix(list(matrix(0, nrow = d, ncol = d)), nrow = num_components, ncol = 1)
    for (k in seq_len(num_components)) {
      denom <- sum(likelihood_matrix[, k])
      training_data_minus_mean_k <- sweep(training_data, MARGIN = 2, next_c_means[k,])
      m <- matrix(0, nrow = d, ncol = d)
      for (t in seq_len(num_training_points)) {
        m <- m + (likelihood_matrix[t, k] * (training_data_minus_mean_k[t,] %*% t(training_data_minus_mean_k[t,])))
      }
      m <- m / denom
      next_c_cov_matrices[[k]] <- m
    }
    next_c_cov_matrices <- saturate_cov_matrices(next_c_cov_matrices, .debug = .debug)

    # check parameters convergence (stop condition)
    if (
      all(abs(c_alphas - next_c_alphas) < stop_diff) &&
        all(abs(c_means - next_c_means) < stop_diff) &&
        all(sapply(seq_len(num_components), function(k) {
          all(abs(c_cov_matrices[[k]] - next_c_cov_matrices[[k]]) < stop_diff)
        }))
    ) {
      convergence <- TRUE
      cat(paste0("[gmm_em_train] EM convergence (diff < ", stop_diff, ") reached in step=", step, "\n"))
      break
    }

    # continue with next iteration
    c_alphas <- next_c_alphas
    c_means <- next_c_means
    c_cov_matrices <- next_c_cov_matrices
    step <- step + 1

  }

  model <- list()

  model$dim <- d
  model$num_components <- num_components
  model$alphas <- c_alphas
  model$means <- c_means
  model$cov_matrices <- c_cov_matrices
  # the returned parameters satisfy the following conditions:
  # 1. model$num_components == length(model$alphas) == nrow(model$means) == nrow(model$cov_matrices)
  # 2. model$dim == ncol(model$means) == nrow(model$cov_matrices[[k]]) == ncol(model$cov_matrices[[k]])
  #    where k is in range 1:model$num_components
  model$em_num_steps <- step
  model$em_covergence <- convergence

  return(model)

}

#' Gaussian mixture model (GMM) with Expectation Maximization (EM) algorithm.
#'
#' Estimates the densities of the given data using the trained GMM model from [gmm_em_train()].
#'
#' @param x matrix (num_sample_points x dim) of data points where the probability density function should be evaluated
#' @param GMM model returned from gmm_em_train
#'
#' @return a vector (of size num_sample_points) of estimated densities evaluated at values given by x
#'
gmm_estimate <- function(x, model) {

  num_samples <- nrow(x)
  d <- ncol(x)

  if (model$dim != d) {
    stop("the dimension of the given data (number of columns in x) does not match the dimension of the given model")
  }

  estimates <- rep(0, times = num_samples)

  for (k in seq_len(model$num_components)) {

    estimates <- estimates + model$alphas[k] * gmm(
      x = x,
      mean = model$means[k,],
      cov_matrix = model$cov_matrices[[k]]
    )

  }

  return(estimates)

}

#' Gaussian component.
#'
#' @param x matrix (num_sample_points x dim) of data points where the probability density function should be evaluated
#' @param mean vector of size dim
#' @param cov_matrix covariance matrix (dim x dim)
#'
#' @return vector (of size num_sample_points) of densities evaluated at values given by x
#'
gmm <- function(x, mean, cov_matrix) {
  d <- ncol(x)
  cov_matrix_det <- det(cov_matrix)
  x_minus_mean <- sweep(x, MARGIN = 2, mean)
  denominator <- sqrt(((2 * pi)^d) * cov_matrix_det)
  # https://stackoverflow.com/questions/11995832/inverse-of-matrix-in-r
  # i1 <- MASS::ginv(c1)
  # i2 <- matlib::inv(c1)
  # i3 <- solve(c1)
  cov_matrix_inv <- solve(cov_matrix)
  numerator <- exp(-colSums(cov_matrix_inv %*% t(x_minus_mean) * t(x_minus_mean)) / 2)
  return(numerator / denominator)
}

saturate_cov_matrices <- function(matrices, .debug = FALSE) {
  for (k in seq_len(nrow(matrices))) {
    m <- matrices[[k]]
    m_det <- det(m)
    if (m_det < 10e-10) {
      matrices[[k]] <- m + diag(10e-5, nrow = nrow(m), ncol = ncol(m))
      if (.debug) {
        cat(paste0("[saturate_cov_matrices] matrix ", k, " has to be saturated\n"))
      }
    }
  }
  return(matrices)
}

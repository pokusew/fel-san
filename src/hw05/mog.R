#' Gaussian mixture model (GMM) with Expectation Maximization (EM) algorithm.
#'
#' Trains GMM model using EM algorithm.
#'
#' @param training_data training data (num_training_points x dim)
#' @param num_components number of Gaussian components (hyperparameter)
#'
#' @seealso https://people.csail.mit.edu/rameshvs/content/gmm-em.pdf
#' @seealso https://en.wikipedia.org/wiki/Mixture_model#Multivariate_Gaussian_mixture_model
#' @seealso https://brilliant.org/wiki/gaussian-mixture-model/
#'
#' @return model, a list with num_components, alphas, means, cov_matrices
#'
gmm_em_train <- function(training_data, num_components, num_steps = 60) {

  d <- dim(training_data)[2]
  num_training_points <- dim(training_data)[1]

  # initilization step
  c_alphas <- rep(1 / num_components, times = num_components)
  c_means <- training_data[sample(seq_len(num_training_points), size = num_components, replace = FALSE),]
  c_cov_matrices <- matrix(list(cov(training_data)), nrow = num_components, ncol = 1)
  c_cov_matrices <- saturate_cov_matrices(c_cov_matrices)

  # likelihood_matrix[t, k] = the probability that x_t is generated by component C_k
  likelihood_matrix <- matrix(0, nrow = num_training_points, ncol = num_components)

  # TODO: instead of hardcoded steps we could probably implement end condition (EM algorithm convergence)
  for (step in seq_len(num_steps)) {

    # expectation (E) step
    for (k in seq_len(num_components)) {
      p_num <- c_alphas[k] * gmm(x = training_data, mean = c_means[k,], cov_matrix = c_cov_matrices[[k]])
      p_denom <- rep(0, times = num_training_points)
      for (j in seq_len(num_components)) {
        pp <- c_alphas[j] * gmm(x = training_data, mean = c_means[j,], cov_matrix = c_cov_matrices[[j]])
        p_denom <- p_denom + pp
      }
      p <- p_num / p_denom
      likelihood_matrix[, k] <- p
    }

    # maximization (M) step
    # note: the order of comuptation is important here
    # 1. alphas
    c_alphas <- colMeans(likelihood_matrix)
    # 2. means
    # note: the following for could be probably vectorized
    for (k in seq_len(num_components)) {
      c_means[k,] <- colSums(sweep(training_data, MARGIN = 1, STATS = likelihood_matrix[, k], FUN = `*`)) / sum(likelihood_matrix[, k])
    }
    # 3. covariance matrices
    for (k in seq_len(num_components)) {
      denom <- sum(likelihood_matrix[, k])
      training_data_minus_mean_k <- sweep(training_data, MARGIN = 2, c_means[k,])
      m <- matrix(0, nrow = d, ncol = d)
      for (t in seq_len(num_training_points)) {
        m <- m + (likelihood_matrix[t, k] * (training_data_minus_mean_k[t,] %*% t(training_data_minus_mean_k[t,])))
      }
      m <- m / denom
      c_cov_matrices[[k]] <- m
    }
    c_cov_matrices <- saturate_cov_matrices(c_cov_matrices)


  }

  model <- list()

  model$num_components <- num_components
  model$alphas <- c_alphas
  model$means <- c_means
  model$cov_matrices <- c_cov_matrices

  return(model)

}

#' Gaussian mixture model (GMM) with Expectation Maximization (EM) algorithm.
#'
#' Trains GMM model using EM algorithm.
#'
#' @param x matrix (num_sample_points x dim) of data points where the probability density function should be evaluated
#' @param GMM model returned from gmm_em_train
#'
#' @return a vector (of size num_sample_points) of estimated densities evaluated at values given by x
#'
gmm_estimate <- function(x, model) {

  num_samples <- dim(x)[1]

  # TODO: asset that the x dimensions match the given model

  estimates <- c(0, times = num_samples)

  for (k in seq_len(model$num_components)) {

    estimates <- estimates + model$alphas[k] * gmm(
      x = x,
      mean = model$means[k,],
      cov_matrix = model$cov_matrices[[k]]
    )

  }

  return (estimates)

}

#' Gaussian component.
#'
#' @param x matrix (num_sample_points x dim) of data points where the probability density function should be evaluated
#' @param mean vector of size dim
#' @param cov_matrix convariance matrix (dim x dim)
#'
#' @return vector (of size num_sample_points) of densities evaluated at values given by x
#'
gmm <- function(x, mean, cov_matrix) {
  d <- dim(x)[2]
  cov_matrix_det <- det(cov_matrix)
  x_minus_mean <- sweep(x, MARGIN = 2, mean)
  denominator <- sqrt(((2 * pi)^d) * cov_matrix_det)
  # https://stackoverflow.com/questions/11995832/inverse-of-matrix-in-r
  # i1 <- MASS::ginv(c1)
  # i2 <- matlib::inv(c1)
  # i3 <- solve(c1)
  cov_matrix_inv <- solve(cov_matrix)
  numerator <- exp(-colSums(cov_matrix_inv %*% t(x_minus_mean) * t(x_minus_mean)) / 2)
  return(numerator / denominator)
}

saturate_cov_matrices <- function(matrices) {
  for (k in seq_len(dim(matrices)[1])) {
    m <- matrices[[k]]
    m_det <- det(m)
    if (m_det < 10e-10) {
      matrices[[k]] <- m + diag(10e-5, nrow = dim(m)[1], ncol = dim(m)[2])
    }
  }
  return(matrices)
}

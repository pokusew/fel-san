{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SAN assignment - Dimensionality reduction\n",
    "\n",
    "Author : Your Name \\\n",
    "Email  : you@fel.cvut.cz"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ad170f",
   "metadata": {
    "name": "setup",
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse.csgraph import shortest_path\n",
    "from scipy.spatial import voronoi_plot_2d, Voronoi, cKDTree\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import MDS, TSNE\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c343d4d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The goal of this tutorial is to get familiar with some basic methods for dimensionality\n",
    "reduction, complete you own implementation of the **Isomap algorithm** (in cooperation with *Multidimensional scaling*), experiment with its parameters and compare with other techniques of dimensionality reduction (**PCA**, **t-SNE**).\n",
    "\n",
    "## Background\n",
    "\n",
    "The data you will be working with are vector representations of words in a latent (unknown) high-dimensional space. This representation of words, also know as word embedding, differs from standard bag-of-words (BoW, TFIDF, etc.) representations in that the meaning of the words is distributed across all the dimensions. Generally speaking, the word embedding algorithms seek to learn a mapping projecting the original BoW representation (simple word index into a given vocabulary) into a lower-dimensional (but still too high for our case) continuous vector-space, based on their distributional properties observed in some raw text corpus. This distributional semantics approach to word representations is based on the basic idea that linguistic items with similar distributions typically have similar meanings, i.e. words that often appear in a similar context (words that surround them) tend to have similar (vector) representations.\n",
    "\n",
    "Speciffically, the data you are presented with are vector representations coming from the most popular algorithm for word embedding known as word2vec[^1] by Tomas Mikolov (VUT-Brno alumni). *Word2vec* is a (shallow) neural model learning the projection of BoW word representations into a latent space by the means of gradient descend. Your task is to further reduce the dimensionality of the word representations to get a visual insight into what has been learned.\n",
    "\n",
    "[^1]: Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In *Advances in neural information processing systems*, pages 3111-3119, 2013.\n",
    "\n",
    "## Data\n",
    "You are given 300-dimensional word2vec vector embeddings in the file *data.csv* with corresponding word labels in *labels.txt* for each line. Each of these words comes from one of 10 selected classes of synonyms, which can be recognized (and depicted) w.r.t. labels denoted in the file *colors.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_points(X, labels, colors, title):\n",
    "    voronoi = Voronoi(X)\n",
    "    fig, ax = plt.subplots(figsize=(10,8), dpi=128)\n",
    "    voronoi_plot_2d(voronoi, ax, show_points=False, show_vertices=False)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=colors, s=8, cmap=mpl.colormaps[\"tab10\"])\n",
    "    for point, label in zip(X, labels):\n",
    "        ax.text(point[0], point[1], label[0], va = \"top\", ha = \"center\", fontsize = \"xx-small\")\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_classification(X, colors, n_folds=10):\n",
    "    accuracy = []\n",
    "    skf = KFold(n_splits=n_folds, shuffle=True, random_state=0)\n",
    "    for train_idxs, test_idxs in skf.split(X, colors):\n",
    "      tree_model = DecisionTreeClassifier().fit(X[train_idxs], colors[train_idxs])\n",
    "      prediction = tree_model.predict(X[test_idxs])\n",
    "      accuracy.append(np.mean(prediction == colors[test_idxs, 0]))\n",
    "    return accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "6daae20d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Tasks\n",
    "1. **Load the dataset of 165 words**, each represented as a 300-dimensional vector. Each word is assigned to one of 10 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Load the dataset of 165 words\n",
    "my_data = pd.read_csv(\"data.csv\", header=None).values\n",
    "my_labels = pd.read_csv(\"labels.txt\", header=None).values\n",
    "my_colors = pd.read_csv(\"colors.csv\", header=None).values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "4fe764b2",
   "metadata": {},
   "source": [
    "The data is in the matrix `mydata`, cluster assignment in `mycolors` and the actual words (useful for visualization) in `mylabels`. You can plot the data by using only the first 2 dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f74d537",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_points(my_data[:, [0,1]], my_labels, my_colors, title=\"First two dimensions of raw data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815c1768",
   "metadata": {},
   "source": [
    "2. **Implement ISO-MAP dimensionality reduction procedure**.\n",
    "  * Use *k*-NN method to construct the neighborhood graph (sparse matrix).\n",
    "    - For simplicity, you can use `cKDTree` method available in `sklearn` package.\n",
    "  * Compute shortest-paths (geodesic) matrix using your favourite algorithm.\n",
    "    - Tip: you can use `shortest_path` method from scipy.\n",
    "  * Project the geodesic distance matrix into 2D space with (Classical) Multidimensional Scaling (`MDS` method from sklearn).\n",
    "  * Challenge: you may simply use PCA to do the same, but be careful to account for a proper normalization (centering) of the geodesic (kernel) matrix (see Kernel PCA for details).\n",
    "  \n",
    "An expected result (for *k* = 5) should look similar (not necessarily exactly the same) to following\n",
    "\n",
    "![Example output](graph_iso.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d759bca0",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "def isomap(data, k):\n",
    "    \"\"\"\n",
    "    Your function computing the ISOMAP embedding for data.\n",
    "\n",
    "    :param data: numpy array (m, d) where m is number of samples and d is the dimension of input space\n",
    "    :param k: number of neighbours to consider for constructing the neighborhood graph\n",
    "    :return: data embedded into R^2 acc. to the ISOMAP algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    #ADD YOUR CODE HERE!\n",
    "\n",
    "    #1.Determine the neighbors of each point (k-NN)\n",
    "\n",
    "    #2.Construct the neighborhood graph.\n",
    "    #Each point is connected to other if it is a K nearest neighbor.\n",
    "    #Edge length equal to Euclidean distance.\n",
    "\n",
    "    #3.Compute the shortest path between two nodes. (Floyd-Warshall algorithm)\n",
    "\n",
    "    #4.(classical) multidimensional scaling\n",
    "\n",
    "    projection = None\n",
    "    return projection\n",
    "\n",
    "for k in range(4, 10):\n",
    "    projection_isomap = isomap(my_data, k=k)\n",
    "    plot_points(projection_isomap, my_labels, my_colors, title=f\"ISOMAP (k={k})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6a4cac",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "3. **Visually compare PCA, ISOMAP and t-SNE** by plotting the word2vec data, embedded into 2D using the `Plotpoints` function. Try finding the optimal *k* value for ISOMAP's nearest neighbour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb5a1ea",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Principal component analysis\n",
    "scaled_data = StandardScaler().fit_transform(my_data)\n",
    "projection_pca = PCA(2).fit_transform(my_data)\n",
    "plot_points(projection_pca[:, [0, 1]], my_labels, my_colors, title=\"PCA\")\n",
    "\n",
    "#t-SNE (T-Distributed Stochastic Neighbor Embedding)\n",
    "projection_tsne = TSNE(n_components=2, perplexity=10, init=\"random\", learning_rate=\"auto\").fit_transform(my_data)\n",
    "plot_points(projection_tsne, my_labels, my_colors, title=\"t-SNE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9ca9c6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "4. **Observe the effect of dimensionality reduction on a classiffication algorithm**. The supporting code in a function `test_classification` performs training and testing of classification trees and gives the classification accuracy (percentage of correctly classified samples) on individual cross-validation test splits as its result. Compare the accuracy of prediction on plain data, PCA, ISOMAP and t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17e760e",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#classify ISOMAP\n",
    "acc_isomap = test_classification(projection_isomap, my_colors, 50)\n",
    "\n",
    "#classify PCA\n",
    "acc_pca = test_classification(projection_pca, my_colors, 50)\n",
    "\n",
    "#classify t-SNE\n",
    "acc_tsne = test_classification(projection_tsne, my_colors, 50)\n",
    "\n",
    "#PLOT results\n",
    "print(f\"ISOMAP ACC: {np.mean(acc_isomap):>10,.3f}\")\n",
    "print(f\"PCA ACC: {np.mean(acc_pca):>13,.3f}\")\n",
    "print(f\"t-SNE ACC: {np.mean(acc_tsne):>11,.3f}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "Python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

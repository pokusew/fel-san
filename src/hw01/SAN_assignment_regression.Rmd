---
title: "SAN Assignment - regression"
author: "Martin Endler <endlemar@fel.cvut.cz>"
output:
pdf_document: default
---

## Submission
Fill in your name above for clarity.
To solve this homework, simply write your answers into this document and fill in the marked pieces of code.
Submit your solution consisting of both this modified Rmd file and a knitted PDF document as an archive to the courseware BRUTE upload system for the SAN course.
The deadline is specified there.

## Initialization

Load the required libraries `gtools`, `caret` and `glmnet`, make sure you have those installed. We also fix the random seed for reproducibility convenience.

```{r}
require(gtools);
require(caret);
require(glmnet);
set.seed(0)
```

Here, we define constants of the assignment.
You may play with the values and observe what happens,
but in your solution you should use the given values unchanged.

```{r}
n.samples <- 256 # Total number of samples (training and testing together)
n.dimensions <- 100 # Number of n.dimensions, a.k.a. attributes or features
```

### Model evaluation procedure

The function `learnAndTest` takes a matrix of independent variables values `X` and a corresponding vector
of dependent variable (a.k.a. response) `Y` then trains and evaluates a model specified by the `modelType` parameter.
If you are interested in all possible `modelType` parameter values, refer to [http://topepo.github.io/caret/train-models-by-tag.html](http://topepo.github.io/caret/train-models-by-tag.html).
Here we will only be using three: `"lm"` for ordinary least squares and `"glmnet"` with parameter `alpha = 1` resp. `alpha = 0` for LASSO resp. Ridge. (There are also `lasso` and `ridge` methods you may try, but these have inconsistent API for passing lambda.) This function learns the model from the data and estimates its accuracy using cross validation. The results are then printed to output. For purpose of this assignment, consider only the RMSE (root-mean-square error) criterion.

```{r}
learnAndTest <- function(X, Y, modelType, ...) {
	data <- data.frame(X, Y)

	train.control <- trainControl(method = "cv", number = 10) # 10-fold cross-validataion
	# Alternative to get more accurate, but slower evaluation:
	# train.control <- trainControl(method = "LOOCV")

	# Here we train the model using the versatile `train` function from the caret package
	train(Y ~ .,
		  data = data,
		  method = modelType,
		  trControl = train.control,
		  ...
	)

}
```

We will also precompute an array of candidate lambda values for LASSO and Ridge.
```{r}
lambda_lasso <- expand.grid(lambda = 10^seq(10, -3, length = 10), alpha = 1)
lambda_ridge <- expand.grid(lambda = 10^seq(10, -3, length = 10), alpha = 0)
```

### Initial data generation

Here, we generate some data.
```{r}
# Generates independent variables by uniform i.i.d. sampling
X <- replicate(
	n.dimensions,
	runif(n.samples, min = -10, max = 10)
)

# Randomly generates the actual underlying coefficients of the linear dependency
coefs <- runif(n.dimensions, min = 1, max = 4)
intercept <- 0 # For simplicity

# Synthesizes dependent variable (observed values) by the given linear dependency plus noise
noise <- rnorm(n.samples, sd = 8, mean = 0) # Gaussian noise to be added to the response
Y <- (X %*% coefs) + intercept + noise # Note: (%*%) is the matrix multiplication operator
```



## Testing the models

Now let us run the following tests:

```{r}
print(learnAndTest(X, Y, "lm"))
print(learnAndTest(X, Y, "glmnet", tuneGrid = lambda_ridge))
print(learnAndTest(X, Y, "glmnet", tuneGrid = lambda_lasso))
```
### Task 1:
**Answer the following questions:**

  * What change in the learned model would you anticipate if we changed the `mean` parameter value to a different constant in the noise generation? You may answer this either by talking about the coefficients or by giving a geometrical interpretation. **Answer:** From a geometric point of view, the predicted y values would be shifted up (for a positive mean), resp. down (for a negative mean). This shift would correspond to a change in the estimated intercept parameter. The other estimated coefficients would remain the same.
  * In our example we generated samples (i.e. the independent variables `X`) from uniform distribution. The least squares  method, on the other hand, has something called the "normality assumption". Have we violated that assumption? Justify. **Answer:** No, we haven't. The "normality assumption" requires that the residuals are normally distributed, not the data (our generated samples).
  * Which method gave the best results? Is it the most common one to do so if you re-run the test several times? Why do you think it performs better the best? **Answer:** The LASSO gave the best result (smallest RMSE) most often while re-running the test multiple times. However, the plain Least squares method was also very good, with a small RMSE very close to the one from LASSO. From the very small lambda parameter in the best LASSO, we can see that the LASSO is almost the same as the Least squares method (the shrinkage penalty term is close to zero). This could mean that the Least squares method is not overfitting on our data.
  * Check the selected values for the lambda parameter for ridge and LASSO. Are they low or high? How does it relate to the above answer? **Answer:** They are both very low. That means that both ridge and LASSO are almost the same as the Least squares method (the shrinkage penalty term is close to zero).

## Least squares assumptions

The data generation model assumed by the ordinary least squares method can be mathematically written as follows:
$$Y = \mathbf{X}^T \boldsymbol{\beta} + \beta_0 + G, \; G \sim \mathcal{N}(0, \sigma^2)$$
This formula implicitly expresses some of the assumptions about the data,
required for the method to work reliably.

  * The observed value $Y$ is influenced by some Gaussian noise $G$.
  * There truly exists an underlying linear dependency.
  * The noise is homoscedastic ($\sigma^2$ is a constant).

### Task 2:
First of all, make sure you understand how elements of this formula correspond to the code in the "data generation" section.

Your task is to violate each of these assumptions (one at a time) and **briefly** comment the changes in the learned model by statistically comparing it to model using the above data. (Coefficient summary below.) The catch here is that you are allowed to only modify the noise generation procedure to achieve that. Attempt to find a way of violating the assumptions to achieve a clear difference, but any solution that is technically correct will be awarded full points.

It is sufficient to look at the `summary` of the OLS model.

```{r}
summary(learnAndTest(X, Y, "lm"))
```


#### Violate noise normality
```{r}
noise2 <- rbeta(n.samples, 1 / 7, 1 / 7)
# noise3 <- ((noise2 - mean(noise2))/sd(noise2) * 8)
noise3 <- ((seq(n.samples) - mean(seq(n.samples))) / sd(seq(n.samples))) * 8

noise2 <- runif(n.samples, min = -8, max = 8)
noise <- rnorm(n.samples, sd = 8, mean = 0)
mean(noise3)
var(noise3)
plot(noise)
plot(noise3)
D3 <- (X %*% coefs) + intercept + noise3
D1 <- (X %*% coefs) + intercept + noise
plot(D1)
plot(D3)

# KEEP THE CODE BELOW
Y <- (X %*% coefs) + intercept + noise3
# summary(learnAndTest(X, Y, "lm"))
test_model <- lm(Y ~ X)
summary(test_model)
# plot(test_model)
```
**Your comment:** We violated noise normality by replacing the normal distribution with uniform distribution (with the 0 mean and


#### Violate linearity
```{r}
noise <- X^4 %*% coefs

# KEEP THE CODE BELOW
Y <- (X %*% coefs) + intercept + noise
summary(learnAndTest(X, Y, "lm"))
```
**Your comment:** We violated the linearity by adding a non-linear term (the fourth power, `X^4 %*% coefs`). The F-statistic is almost equal to 1, which means that the model has no predictive capability (i.e., it is as good as an intercept-only model).

#### Violate homoscedasticity
```{r}
# seq(from = 2, to = 32, length.out = n.samples)
Y_without_noise <- (X %*% coefs) + intercept
Y_normalized_01 <- (Y_without_noise - min(Y_without_noise)) / (max(Y_without_noise) - min(Y_without_noise))
variable_sd <- Y_normalized_01 * 32
noise <- rnorm(n.samples, sd = seq(from = 2, to = 32, length.out = n.samples), mean = 0)
plot(noise)

# KEEP THE CODE BELOW
Y <- (X %*% coefs) + intercept + noise
hd_model <- learnAndTest(X, Y, "lm")
hd_model.redisdual_plot_data <- data.frame(
	my_res = hd_model$finalModel$residuals,
	my_y = hd_model$finalModel$fitted.values
)
ggplot(hd_model.redisdual_plot_data, aes(x = my_y, y = my_res)) +
	geom_point(size = 2)
summary(hd_model)
```
**Your comment:** We violated the homoscedasticity by making the variance non-constant.

## Understanding the advantages of shrinkage methods

In this part, we will be modifying the dependent variables `X` from task 1 by a linear transformation, represented by a square matrix of `n.dimensions` sides.
For demonstration, consider that the identity function represented by the identity matrix:
```{r}
M0 <- diag(n.dimensions)     # Makes an identity matrix
X0 <- X %*% M0              # You can check that X0 == X
noise <- rnorm(n.samples, sd = 8, mean = 0)
Y0 <- (X0 %*% coefs) + noise # We can reuse the noise as it's independent of X.
```

### Task 3

In this task, you will show your understanding of the advantages of Ridge by synthesizing data on which they perform the best. You are supposed do this by linearly transforming the dataset `X` as in the example above. In other words, you should construct a matrix which if used in place of `M0` in the above example would make Ridge perform better than the other two methods. You should not resort to degenerate cases where you would get a warning about using a rank-deficient matrix. Justify your method.

**Scoring note:** The difference in the RMSE criterion doesn't need to be large or does not need to be present if you are certain it's just an statistical artifact (which you could verify by re-running the tests multiple times or using the LOOCV in `learnAndTest` function). Your design and justification is what matters for the assignment evaluation and the measurements are here only to guide you. We expect this task to be challenging to students, but once you get the right idea, it is possible to implement it with very small amount of code.

#### Ridge
```{r}
M1 <- diag(n.dimensions) # REPLACE THIS WITH YOUR CODE

# KEEP THE CODE BELOW
noise <- rnorm(n.samples, sd = 8, mean = 0)
X1 <- X %*% M1
Y1 <- (X1 %*% coefs) + noise
```

Running these test should now make Ridge perform the best.
```{r}
print(learnAndTest(X1, Y1, "lm"))
print(learnAndTest(X1, Y1, "ridge"))
print(learnAndTest(X1, Y1, "lasso"))
```
**Justification:**


#### LASSO (OPTIONAL CHALLENGE)
This part of the homework is purely optional, but we are eager to see students capable of solving this.
Here you should do the same thing as above, but to make perform LASSO the best of the three methods.
Although similar in nature, we consider this even more challenging than the above
since being unable to modify the underlying coefficients,
this may require some deeper considerations to justify the transformation method.
It can still be implemented with a few short lines of code, though.

```{r}
M2 <- diag(n.dimensions) # REPLACE THIS WITH YOUR CODE

# KEEP THE CODE BELOW
noise <- rnorm(n.samples, sd = 8, mean = 0)
X2 <- X %*% M2
Y2 <- (X2 %*% coefs) + noise
```

Running these test should now make LASSO perform the best.
```{r}
print(learnAndTest(X2, Y2, "lm"))
print(learnAndTest(X2, Y2, "ridge"))
print(learnAndTest(X2, Y2, "lasso"))
```
**Justification:**

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e47d6ef",
   "metadata": {},
   "source": [
    "# SAN Assignment - regression\n",
    "\n",
    "Author : Your Name \\\n",
    "Email  : you@fel.cvut.cz"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Submission\n",
    "Fill in your name above for clarity.\n",
    "To solve this homework, simply write your answers into this document and fill in the marked pieces of code.\n",
    "Submit your solution consisting of both this modified notebook file and exported PDF document as an archive to the courseware BRUTE upload system for the SAN course.\n",
    "The deadline is specified there. \n",
    "\n",
    "## Initialization\n",
    "\n",
    "Load the required libraries `numpy`, `statsmodels` and `sklearn`, make sure you have those installed. We also fix the random seed for reproducibility convenience."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sma\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "seed = 0\n",
    "\n",
    "np.random.seed(seed)\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "rs = np.random.RandomState(seed=seed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, we define constants of the assignment.\n",
    "You may play with the values and observe what happens,\n",
    "but in your solution you should use the given values unchanged."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "n_samples = 256     # Total number of samples (training and testing together)\n",
    "n_dimensions = 100  # Number of n.dimensions, a.k.a. attributes or features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model evaluation procedure\n",
    "\n",
    "The function `learn_and_test` takes a matrix of independent variables values `X` and a corresponding vector\n",
    "of dependent variable (a.k.a. response) `y` then trains and evaluates a model specified by the `modelType` parameter.\n",
    "\n",
    "Unlike in R, in Python you have much higher control over your models and have to be explicit. That is why this function is prepared for you specially for this assignment and is prepared to provide you interface to three simple models---`\"ols\"` for ordinary least squares, `\"lasso\"` for LASSO and `\"ridge\"` for Ridge regression---including the cross-validation to select the best regularization parameter alpha for the regularized models. \n",
    "The models all come from the statsmodels package and you can refer to their documentation at https://www.statsmodels.org, the models are specific instants of the OLS model.\n",
    "For purpose of this assignment, consider only the RMSE (root-mean-square error) criterion. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def learn_and_test(X, y, model_type, alphas=None):\n",
    "    assert model_type in [\"ols\", \"lasso\", \"ridge\"]\n",
    "\n",
    "    if model_type == \"ols\":\n",
    "        res = sma.OLS(endog=y, exog=X).fit()\n",
    "        print(res.summary())\n",
    "    else:\n",
    "        assert alphas is not None\n",
    "\n",
    "        n_splits = 10\n",
    "        print()\n",
    "        print(\" \" * 10 + \"GLS regression results\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Model type:  {model_type}\")\n",
    "        print(f\"Cross-validation results ({n_splits} splits)\")\n",
    "        print(r\"     alpha           MSE         R2          MAE\")\n",
    "\n",
    "        skf = KFold(n_splits=n_splits, shuffle=True, random_state=rs)\n",
    "        mses_over_alphas = []\n",
    "        for a in alphas:\n",
    "            total_mse = 0\n",
    "            total_r2 = 0\n",
    "            total_mae = 0\n",
    "            for train_idxs, test_idxs in skf.split(X, y):\n",
    "                model = sma.OLS(endog=y[train_idxs], exog=X[train_idxs])\n",
    "                fit = model.fit_regularized(alpha=a, L1_wt=1 if model_type == \"lasso\" else 0)\n",
    "                cmp = y[test_idxs], fit.predict(X[test_idxs])\n",
    "                total_mse += mean_squared_error(*cmp)\n",
    "                total_mae += mean_absolute_error(*cmp)\n",
    "                total_r2 += r2_score(*cmp)\n",
    "            mean_mse = total_mse / skf.get_n_splits()\n",
    "            mean_r2 = total_r2 / skf.get_n_splits()\n",
    "            mean_mae = total_mae / skf.get_n_splits()\n",
    "            print(f\"  {a: >10,.4e}  |  {mean_mse: >10,.3f}   {mean_r2:>6,.8f}   {mean_mae:>10,.3f}\")\n",
    "\n",
    "            mses_over_alphas.append(mean_mse)\n",
    "        best = np.argmin(mses_over_alphas)\n",
    "        res = sma.GLS(endog=y, exog=X).fit_regularized(alpha=alphas[best], L1_wt=1 if model_type == \"lasso\" else 0)\n",
    "\n",
    "    return res\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will also precompute an array of candidate alpha values for LASSO and Ridge."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "alphas = 10 ** np.linspace(10, -3, num=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initial data generation\n",
    "\n",
    "Here, we generate some data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Generates independent variables by uniform i.i.d. sampling\n",
    "X = rng.uniform(low=-10, high=10, size=(n_samples, n_dimensions))\n",
    "\n",
    "# Randomly generates the actual underlying coefficients of the linear dependency\n",
    "coefs = rng.uniform(low=1, high=4, size=(n_dimensions,)) \n",
    "intercept = 0 # For simplicity\n",
    "\n",
    "# Synthesizes dependent variable (observed values) by the given linear dependency plus noise\n",
    "noise = rng.normal(loc = 0, scale = 8, size=(n_samples)) # Gaussian noise to be added to the response\n",
    "y = (X @ coefs) + intercept + noise"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the models\n",
    "\n",
    "Now let us run the following tests. Note, that in Python implementation of the $R^2$ score, the computed value can be negative, which means that the trained model is worse than uninformed one. Reflect on why that can happen and why do we see this for the regularized models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                      y   R-squared (uncentered):                   0.999\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.998\n",
      "Method:                 Least Squares   F-statistic:                              1065.\n",
      "Date:                Mon, 10 Oct 2022   Prob (F-statistic):                   3.06e-186\n",
      "Time:                        15:19:01   Log-Likelihood:                         -818.96\n",
      "No. Observations:                 256   AIC:                                      1838.\n",
      "Df Residuals:                     156   BIC:                                      2192.\n",
      "Df Model:                         100                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             3.4188      0.105     32.629      0.000       3.212       3.626\n",
      "x2             3.7885      0.102     37.001      0.000       3.586       3.991\n",
      "x3             3.4158      0.108     31.719      0.000       3.203       3.629\n",
      "x4             1.0754      0.098     10.951      0.000       0.881       1.269\n",
      "x5             1.8442      0.098     18.863      0.000       1.651       2.037\n",
      "x6             3.9223      0.103     38.031      0.000       3.719       4.126\n",
      "x7             1.6228      0.102     15.865      0.000       1.421       1.825\n",
      "x8             4.0534      0.108     37.589      0.000       3.840       4.266\n",
      "x9             3.2440      0.109     29.802      0.000       3.029       3.459\n",
      "x10            2.0576      0.097     21.184      0.000       1.866       2.249\n",
      "x11            3.3600      0.099     33.821      0.000       3.164       3.556\n",
      "x12            2.6576      0.106     25.087      0.000       2.448       2.867\n",
      "x13            2.1762      0.106     20.571      0.000       1.967       2.385\n",
      "x14            2.6063      0.106     24.524      0.000       2.396       2.816\n",
      "x15            2.7124      0.105     25.796      0.000       2.505       2.920\n",
      "x16            1.4633      0.110     13.245      0.000       1.245       1.681\n",
      "x17            2.0490      0.111     18.539      0.000       1.831       2.267\n",
      "x18            1.6906      0.097     17.421      0.000       1.499       1.882\n",
      "x19            3.0982      0.102     30.483      0.000       2.897       3.299\n",
      "x20            2.9912      0.102     29.386      0.000       2.790       3.192\n",
      "x21            1.1558      0.103     11.170      0.000       0.951       1.360\n",
      "x22            3.3311      0.102     32.553      0.000       3.129       3.533\n",
      "x23            3.1995      0.110     28.978      0.000       2.981       3.418\n",
      "x24            2.9271      0.098     29.776      0.000       2.733       3.121\n",
      "x25            1.9016      0.106     17.970      0.000       1.693       2.111\n",
      "x26            2.9490      0.113     26.179      0.000       2.726       3.172\n",
      "x27            3.2672      0.100     32.597      0.000       3.069       3.465\n",
      "x28            2.0384      0.102     19.987      0.000       1.837       2.240\n",
      "x29            2.9179      0.106     27.496      0.000       2.708       3.128\n",
      "x30            1.8578      0.108     17.258      0.000       1.645       2.070\n",
      "x31            3.7683      0.104     36.232      0.000       3.563       3.974\n",
      "x32            3.5955      0.103     34.754      0.000       3.391       3.800\n",
      "x33            3.6851      0.100     36.669      0.000       3.487       3.884\n",
      "x34            1.1465      0.099     11.544      0.000       0.950       1.343\n",
      "x35            1.3670      0.111     12.270      0.000       1.147       1.587\n",
      "x36            1.5974      0.103     15.532      0.000       1.394       1.801\n",
      "x37            1.9874      0.101     19.599      0.000       1.787       2.188\n",
      "x38            3.9449      0.108     36.637      0.000       3.732       4.158\n",
      "x39            2.5782      0.099     25.954      0.000       2.382       2.774\n",
      "x40            2.9346      0.101     29.168      0.000       2.736       3.133\n",
      "x41            1.8947      0.107     17.642      0.000       1.683       2.107\n",
      "x42            2.2553      0.106     21.221      0.000       2.045       2.465\n",
      "x43            1.2208      0.108     11.260      0.000       1.007       1.435\n",
      "x44            3.3769      0.104     32.499      0.000       3.172       3.582\n",
      "x45            3.7470      0.105     35.602      0.000       3.539       3.955\n",
      "x46            3.6925      0.106     34.904      0.000       3.484       3.902\n",
      "x47            2.3817      0.101     23.478      0.000       2.181       2.582\n",
      "x48            3.1884      0.114     28.055      0.000       2.964       3.413\n",
      "x49            2.1294      0.103     20.613      0.000       1.925       2.333\n",
      "x50            2.6073      0.103     25.243      0.000       2.403       2.811\n",
      "x51            2.0801      0.106     19.533      0.000       1.870       2.290\n",
      "x52            3.2694      0.098     33.273      0.000       3.075       3.463\n",
      "x53            1.6402      0.112     14.683      0.000       1.420       1.861\n",
      "x54            2.0686      0.105     19.615      0.000       1.860       2.277\n",
      "x55            2.6720      0.101     26.335      0.000       2.472       2.872\n",
      "x56            3.8693      0.105     36.880      0.000       3.662       4.077\n",
      "x57            2.8343      0.103     27.531      0.000       2.631       3.038\n",
      "x58            0.9151      0.101      9.033      0.000       0.715       1.115\n",
      "x59            1.4350      0.104     13.766      0.000       1.229       1.641\n",
      "x60            3.3622      0.110     30.612      0.000       3.145       3.579\n",
      "x61            2.2430      0.104     21.551      0.000       2.037       2.449\n",
      "x62            2.3986      0.126     19.012      0.000       2.149       2.648\n",
      "x63            2.3453      0.106     22.136      0.000       2.136       2.555\n",
      "x64            1.9232      0.109     17.567      0.000       1.707       2.139\n",
      "x65            1.4223      0.103     13.817      0.000       1.219       1.626\n",
      "x66            1.8072      0.104     17.345      0.000       1.601       2.013\n",
      "x67            2.8238      0.111     25.471      0.000       2.605       3.043\n",
      "x68            1.8804      0.105     17.922      0.000       1.673       2.088\n",
      "x69            3.0276      0.102     29.755      0.000       2.827       3.229\n",
      "x70            1.7814      0.102     17.490      0.000       1.580       1.983\n",
      "x71            2.8091      0.101     27.848      0.000       2.610       3.008\n",
      "x72            1.7450      0.103     16.868      0.000       1.541       1.949\n",
      "x73            3.2448      0.112     28.982      0.000       3.024       3.466\n",
      "x74            2.9312      0.109     26.958      0.000       2.716       3.146\n",
      "x75            1.9798      0.109     18.174      0.000       1.765       2.195\n",
      "x76            3.1128      0.111     27.966      0.000       2.893       3.333\n",
      "x77            3.0113      0.099     30.562      0.000       2.817       3.206\n",
      "x78            2.0842      0.106     19.617      0.000       1.874       2.294\n",
      "x79            3.7226      0.106     35.145      0.000       3.513       3.932\n",
      "x80            2.1423      0.100     21.422      0.000       1.945       2.340\n",
      "x81            2.6716      0.104     25.807      0.000       2.467       2.876\n",
      "x82            2.8494      0.103     27.793      0.000       2.647       3.052\n",
      "x83            2.2944      0.105     21.776      0.000       2.086       2.502\n",
      "x84            2.4816      0.104     23.910      0.000       2.277       2.687\n",
      "x85            3.1641      0.102     31.064      0.000       2.963       3.365\n",
      "x86            2.5562      0.104     24.468      0.000       2.350       2.763\n",
      "x87            2.2377      0.109     20.582      0.000       2.023       2.452\n",
      "x88            1.5039      0.112     13.384      0.000       1.282       1.726\n",
      "x89            3.3103      0.101     32.799      0.000       3.111       3.510\n",
      "x90            3.0518      0.105     28.957      0.000       2.844       3.260\n",
      "x91            1.5586      0.110     14.109      0.000       1.340       1.777\n",
      "x92            3.2742      0.111     29.501      0.000       3.055       3.493\n",
      "x93            3.2555      0.116     28.051      0.000       3.026       3.485\n",
      "x94            3.1100      0.111     27.894      0.000       2.890       3.330\n",
      "x95            2.0945      0.107     19.545      0.000       1.883       2.306\n",
      "x96            3.2979      0.099     33.224      0.000       3.102       3.494\n",
      "x97            1.8866      0.105     17.963      0.000       1.679       2.094\n",
      "x98            1.1551      0.117      9.911      0.000       0.925       1.385\n",
      "x99            1.4584      0.106     13.780      0.000       1.249       1.667\n",
      "x100           1.5696      0.099     15.778      0.000       1.373       1.766\n",
      "==============================================================================\n",
      "Omnibus:                        6.225   Durbin-Watson:                   2.034\n",
      "Prob(Omnibus):                  0.044   Jarque-Bera (JB):                4.156\n",
      "Skew:                          -0.154   Prob(JB):                        0.125\n",
      "Kurtosis:                       2.457   Cond. No.                         4.06\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
      "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "          GLS regression results\n",
      "==================================================\n",
      "Model type:  lasso\n",
      "Cross-validation results (10 splits)\n",
      "     alpha           MSE         R2          MAE\n",
      "  1.0000e+10  |  24,067.749   -0.04549614      124.269\n",
      "  3.5938e+08  |  24,123.632   -0.05515856      124.406\n",
      "  1.2915e+07  |  24,070.489   -0.03232155      124.314\n",
      "  4.6416e+05  |  24,076.044   -0.02166995      124.262\n",
      "  1.6681e+04  |  24,060.440   -0.04533525      124.245\n",
      "  5.9948e+02  |  24,079.037   -0.04328478      124.276\n",
      "  2.1544e+01  |   4,882.521   0.78553589       53.858\n",
      "  7.7426e-01  |     176.364   0.99230649       10.511\n",
      "  2.7826e-02  |      99.919   0.99504282        8.265\n",
      "  1.0000e-03  |     107.958   0.99488372        8.520\n",
      "\n",
      "          GLS regression results\n",
      "==================================================\n",
      "Model type:  ridge\n",
      "Cross-validation results (10 splits)\n",
      "     alpha           MSE         R2          MAE\n",
      "  1.0000e+10  |  24,019.978   -0.03268345      124.172\n",
      "  3.5938e+08  |  24,048.386   -0.01463359      124.245\n",
      "  1.2915e+07  |  24,067.357   -0.02680934      124.365\n",
      "  4.6416e+05  |  24,068.894   -0.09318598      124.317\n",
      "  1.6681e+04  |  23,912.839   -0.03365360      123.935\n",
      "  5.9948e+02  |  21,458.503   0.08436751      117.343\n",
      "  2.1544e+01  |   5,595.428   0.75656385       58.533\n",
      "  7.7426e-01  |     150.541   0.99285702       10.053\n",
      "  2.7826e-02  |      97.137   0.99530281        7.998\n",
      "  1.0000e-03  |     101.795   0.99551328        8.217\n"
     ]
    },
    {
     "data": {
      "text/plain": "<statsmodels.base.elastic_net.RegularizedResultsWrapper at 0x7f338b2beb50>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_and_test(X, y, \"ols\")\n",
    "learn_and_test(X, y, \"lasso\", alphas = alphas)\n",
    "learn_and_test(X, y, \"ridge\", alphas = alphas)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 1:\n",
    "**Answer the following questions:**\n",
    "\n",
    "  * What change in the learned model would you anticipate if we changed the `mean` parameter value to a different constant in the noise generation? You may answer this either by talking about the coefficients or by giving a geometrical interpretation. **Answer:**  \n",
    "  * In our example we generated samples (i.e. the independent variables `X`) from uniform distribution. The least squares  method, on the other hand, has something called the \"normality assumption\". Have we violated that assumption? Justify. **Answer:** \n",
    "  * Which method gave the best results? Is it the most common one to do so if you re-run the test several times? Why do you think it performs better the best? **Answer:** \n",
    "  * Check the selected values for the lambda parameter for ridge and LASSO. Are they low or high? How does it relate to the above answer? **Answer:** \n",
    "\n",
    "## Least squares assumptions\n",
    "\n",
    "The data generation model assumed by the ordinary least squares method can be mathematically written as follows:\n",
    "$$Y = \\mathbf{X}^T \\boldsymbol{\\beta} + \\beta_0 + G, \\; G \\sim \\mathcal{N}(0, \\sigma^2)$$\n",
    "This formula implicitly expresses some of the assumptions about the data,\n",
    "required for the method to work reliably.  \n",
    "\n",
    "  * The observed value $Y$ is influenced by some Gaussian noise $G$.  \n",
    "  * There truly exists an underlying linear dependency. \n",
    "  * The noise is homoscedastic ($\\sigma^2$ is a constant).\n",
    "  \n",
    "### Task 2:\n",
    "First of all, make sure you understand how elements of this formula correspond to the code in the \"data generation\" section.\n",
    "\n",
    "Your task is to violate each of these assumptions (one at a time) and **briefly** comment the changes in the learned model by statistically comparing it to model using the above data. (Coefficient summary below.) The catch here is that you are allowed to only modify the noise generation procedure to achieve that. Attempt to find a way of violating the assumptions to achieve a clear difference, but any solution that is technically correct will be awarded full points. \n",
    "\n",
    "It is sufficient to look at the `summary` of the OLS model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learn_and_test(X, y, \"ols\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Violate noise normality"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "noise = rng.normal(loc=0, scale=8, size=n_samples) # REPLACE THIS WITH YOUR CODE\n",
    "\n",
    "# KEEP THE CODE BELOW\n",
    "y = X @ coefs + intercept + noise\n",
    "learn_and_test(X, y, \"ols\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Your comment:**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Violate linearity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "noise = rng.normal(loc=0, scale=8, size=n_samples) # REPLACE THIS WITH YOUR CODE\n",
    "\n",
    "# KEEP THE CODE BELOW\n",
    "y = X @ coefs + intercept + noise\n",
    "learn_and_test(X, y, \"ols\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Your comment:** "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Violate homoscedasticity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "noise = rng.normal(loc=0, scale=8, size=n_samples) # REPLACE THIS WITH YOUR CODE\n",
    "\n",
    "# KEEP THE CODE BELOW\n",
    "y = X @ coefs + intercept + noise\n",
    "learn_and_test(X, y, \"ols\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Your comment:** \n",
    "\n",
    "## Understanding the advantages of shrinkage methods \n",
    "\n",
    "In this part, we will be modifying the dependent variables `X` from task 1 by a linear transformation, represented by a square matrix of `n.dimensions` sides. \n",
    "For demonstration, consider that the identity function represented by the identity matrix:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "M0 = np.eye(n_dimensions)                                   # Makes an identity matrix\n",
    "noise = rng.normal(loc=0, scale=8, size=n_samples)          # Sample noise\n",
    "X0 = X @ M0                                                 # You can check that X0 == X\n",
    "Y0 = X0 @ coefs + noise                                     # We can reuse the noise as it's independent of X"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 3\n",
    "\n",
    "In this task, you will show your understanding of the advantages of Ridge by synthesizing data on which they perform the best. You are supposed do this by linearly transforming the dataset `X` as in the example above. In other words, you should construct a matrix which if used in place of `M0` in the above example would make Ridge perform better than the other two methods. You should not resort to degenerate cases where you would get a warning about using a rank-deficient matrix. Justify your method.\n",
    "\n",
    "**Scoring note:** The difference in the RMSE criterion doesn't need to be large or does not need to be present if you are certain it's just an statistical artifact (which you could verify by re-running the tests multiple times or using the LOOCV in `learnAndTest` function). Your design and justification is what matters for the assignment evaluation and the measurements are here only to guide you. We expect this task to be challenging to students, but once you get the right idea, it is possible to implement it with very small amount of code. \n",
    "\n",
    "#### Ridge"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# REPLACE\n",
    "M1 = np.eye(n_dimensions)\n",
    "\n",
    "# KEEP THE CODE BELOW\n",
    "noise = rng.normal(loc=0, scale=8, size=n_samples)\n",
    "X1 = X @ M1\n",
    "Y1 = X1 @ coefs + noise"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Running these test should now make Ridge perform the best. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learn_and_test(X1, Y1, \"ols\")\n",
    "learn_and_test(X1, Y1, \"ridge\", alphas=alphas)\n",
    "learn_and_test(X1, Y1, \"lasso\", alphas=alphas)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Justification:** "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### LASSO (OPTIONAL CHALLENGE)\n",
    "This part of the homework is purely optional, but we are eager to see students capable of solving this. \n",
    "Here you should do the same thing as above, but to make perform LASSO the best of the three methods.\n",
    "Although similar in nature, we consider this even more challenging than the above\n",
    "since being unable to modify the underlying coefficients, \n",
    "this may require some deeper considerations to justify the transformation method.\n",
    "It can still be implemented with a few short lines of code, though."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "R",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

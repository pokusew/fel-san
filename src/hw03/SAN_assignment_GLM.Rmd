---
title: "Generalized Linear Models"
author: "Martin Endler <endlemar@fel.cvut.cz>"
output: html_document
---

#### Introduction
The aim of this assignment is to practice constructing linear models. You will start with a simple linear model. You will evaluate and interpret it (1p). Consequently, your task will be to improve this model using generalized linear models (GLMs) and feature transformations. You will get 1p for proposal and evaluation of GLM (family, evaluation, interpretation), 1p for correct feature transformations, 1p for proposal and justification of the final model and eventually, 1p for comprehensive evaluation of all the model improvements (ablation study through cross-validation, note that the previous evaluations must be done without cross-validation).

#### Input data
In this assignment, you will work with a student dataset. The dataset contains 200 samples and 4 features: *num_awards* is the outcome variable and indicates the number of awards earned by students in a year, *math* is a continuous predictor variable and represents students’ scores on their math final exam, *prog* is a categorical predictor variable with three levels indicating the type of program in which the students were enrolled (1 = “General”, 2 = “Academic” and 3 = “Vocational”), and *work* is a continuous predictor that gives the number of hours that students spent at work on average per week.

#### Load the necessary libraries and the dataset
```{r results = "hide"}
library(dplyr)
library(caret) # comprehensive model evaluation
library(splines)
library(ggplot2) # visualizations
library(rcompanion) # comparison of GLMs

d <- read.csv("study_data.csv")
prog_labels <- c("1: General", "2: Academic", "3: Vocational")
```

#### A few preliminary explorations

```{r}
# hist(
# 	x = d$num_awards,
# 	breaks = length(unique(d$num_awards)),
# 	xlab = "num_awards",
# 	ylab = "no. samples",
# 	main = "Distribution of num_awards"
# )

# distribution of num_awards in our data
# num_awards is a discrete outcome (aka reponse or depdenent) variable
# that represents count data. It is always non-negative.
d %>%
	ggplot(aes(x = num_awards)) +
	geom_histogram(binwidth = 0.5, center = 0) +
	ggtitle("Distribution of num_awards")

# d2 <- d %>% mutate(prog = as.factor(prog))
d2 <- d %>% mutate(prog = factor(prog, labels = prog_labels))

# relationship between math score and num_awards (also considering the program)
d2 %>%
	ggplot(aes(x = math, y = num_awards, shape = prog, color = prog)) +
	geom_jitter(size = 4, width = 0.2, height = 0.2)

# relationship between work (the number of hours that student spent at work on average per week)
# and num_awards (also considering the program)
d2 %>%
	ggplot(aes(x = work, y = num_awards, shape = prog, color = prog)) +
	geom_jitter(size = 4, width = 0.2, height = 0.2)

d2 %>%
	ggplot(aes(x = prog, y = work)) +
	geom_jitter(size = 4, width = 0.2, height = 0.2)

# covariation and correlation matrices might be useful
d.cov <- cov(d)
d.cor <- cor(d)

```

#### Simple linear model
Let us start with an ordinary linear model with no feature transformations. Explain how far the model works (does it meet formal assumptions?, does it overcome the null model?). Which predictors would you keep there and which of them are not useful? Use the standard evaluation procedures that we have for linear models.
```{r}
simple_lm <- lm(num_awards ~ ., d)
summary(simple_lm)

simple_lm_preds <- predict(simple_lm, d)

# What are the shortcomings of the simple regression?
plot(
	x = d$num_awards + runif(nrow(d), -0.2, 0.2),
	y = simple_lm_preds,
	xlab = 'real no. awards',
	ylab = 'predicted no. awards',
	ylim = c(-1, 6)
)

# lm debug plots
par(mfrow = c(2, 2))
plot(simple_lm)

hist(d$num_awards)

# Refer to the lectures what this call measures
anova(lm(num_awards ~ math + prog + work, d))
```
## Add your verbal summary here (1p):


Q-Q plot
normal
outliers data points with id 95, 96 (5 awards), 194 (6 awards)

lognormal/Possionns

##### Generalized linear model
Now, the goal is to implement a generalized linear model that conceptually fits the given task. Do not transform the predictors yet, use them as they are, or omit them from the model. Once you obtain your model, interpret the effect of the *math* predictor on the outcome. How (according to your model) increasing a math score by one point affects the number of awards won?

Explain why the model overcomes the previous linear model, or justify that the generalized model is not needed. Compare the models theoretically as well as technically in terms of a proper quality measure. Note: The difference between the models cannot be statistically tested.
```{r}
# # Step 1:
# num_awards is a discrete outcome (aka reponse or depdenent) variable
# that represents count data. It is always non-negative.
# Let's assess the applicability of Poisson regression.
# Slice the data at each math-points-group and observe the distribution of the count response (num_awards).
bin_size <- 5 # math points
d %>%
	mutate(bin = round(math / bin_size) * bin_size) %>%
	ggplot(aes(x = num_awards)) +
	geom_histogram(binwidth = 0.5, center = 0) +
	facet_grid(cols = vars(bin)) +
	ggtitle("num_awards per math score group")

# Let's try the Poisson regression, which should be better suited - we have count data
# and its distribution seem to be Poisson.

poiss_reg <- glm(num_awards ~ math, family = "poisson", data = d)
summary(poiss_reg)

# predict(poiss_reg, d)
preds <- round(predict(poiss_reg, d, type = "response"))

d %>%
	mutate(preds = preds) %>%
	ggplot(aes(x = math, y = num_awards)) +
	geom_point(size = 1.5, color = 'gray') +
	geom_jitter(size = 1, width = 0.3, height = 0.3, color = 'black') +
	# ggplot can do glm by itself
	geom_smooth(method = 'glm', formula = y ~ x, method.args = list(family = "gaussian")) +
	geom_point(aes(x = math, y = preds, col = "predicted")) +
	theme_minimal()

# Questions> Look at the summary - what is the equation of the model?
#     Knowing the model prescription, think of how can we interpret the coefficients
#       Ask the usual question "How does the output change with a unit increase of the predictor?"
#       The relationship is multiplicative.
#       head_age + 1 -> fs = fs_prev * e^(coef)

```
## Add your verbal summary here (1p):


##### Feature transformations and final model
*prog* and *work* did not prove to be effective predictors so far. Visualize these predictors as well as their relationship with the outcome variable. Based on the observations, propose suitable transformations for them (or, justify that they are truly uninformative for prediction of *num_awards*) and implement them into the best model found by now. Use the *compareGLM()* function to validate that your new GLMs indeed improved over the simple one.
```{r}
typeof(d$prog)

d %>%
	mutate(prog = as.factor(prog)) %>%
	group_by(num_awards, prog) %>%
	summarise(count = n()) %>%
	ggplot(aes(x = num_awards, y = count, fill = prog)) +
	geom_col(position = "dodge") +
	xlab("Number of awards") +
	ylab("no. students")

d %>%
	ggplot(aes(x = work, y = num_awards, color = factor(prog))) +
	geom_point() +
	xlab("Work hours") +
	ylab("Number of awards")

#### add your code here ####

```

## Add your verbal summary here (2p):

##### Ablation study through cross-validation
Recap all the models considered previously and evaluate them through cross-validation. You can start with the most simple null model and gradually add the previously discussed improvements. See their role in terms of MAE, RMSE and other commonly used criteria. The procedure outlined below proposes to work with *train* function from *caret* package, you can only add more models to evaluate and compare.
```{r,warning=FALSE}
train_control <- trainControl(method = "cv", number = 10)

# CV of the null lm model
print(train(
	x = data.frame(rep(1, nrow(d))),
	y = d$num_awards,
	method = "lm",
	trControl = train_control
)$results)

# CV of the simple linear model with all predictors
print('t' + train(
	x = d %>% select(prog, math, work),
	y = d$num_awards,
	method = "lm",
	trControl = train_control
)$results)

# CV of the simple linear model with math predictor only
summary(
	train(
		x = d %>% select(math),
		y = d$num_awards,
		trControl = train_control,
		method = "glm",
		family = "poisson",
	)
)

```
## Add your verbal summary here (1p):


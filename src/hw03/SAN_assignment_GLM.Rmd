---
title: "Generalized Linear Models"
author: "Martin Endler <endlemar@fel.cvut.cz>"
output:
  pdf_document: default
  html_document: default
---

## Introduction
The aim of this assignment is to practice constructing linear models. You will start with a simple linear model. You will evaluate and interpret it (1p). Consequently, your task will be to improve this model using generalized linear models (GLMs) and feature transformations. You will get 1p for proposal and evaluation of GLM (family, evaluation, interpretation), 1p for correct feature transformations, 1p for proposal and justification of the final model and eventually, 1p for comprehensive evaluation of all the model improvements (ablation study through cross-validation, note that the previous evaluations must be done without cross-validation).

## Input data
In this assignment, you will work with a student dataset. The dataset contains 200 samples and 4 features: *num_awards* is the outcome variable and indicates the number of awards earned by students in a year, *math* is a continuous predictor variable and represents students’ scores on their math final exam, *prog* is a categorical predictor variable with three levels indicating the type of program in which the students were enrolled (1 = “General”, 2 = “Academic” and 3 = “Vocational”), and *work* is a continuous predictor that gives the number of hours that students spent at work on average per week.

## Load the necessary libraries and the dataset
```{r results = "hide"}
library(dplyr)
library(caret) # comprehensive model evaluation
library(splines)
library(ggplot2) # visualizations
library(rcompanion) # comparison of GLMs

d <- read.csv("study_data.csv")
prog_labels <- c("1: General", "2: Academic", "3: Vocational")
```

## A few preliminary explorations

```{r}
# distribution of num_awards in our data
# (so that we have an idea of what we are dealing with)
# num_awards is a discrete outcome (aka reponse or depdenent) variable
# that represents count data. It is always non-negative.
d %>%
	ggplot(aes(x = num_awards)) +
	geom_histogram(binwidth = 0.5, center = 0) +
	ggtitle("Distribution of num_awards")

# d2 - copy of data (d) where prog is a factor
typeof(d$prog)
is.factor(d$prog)
# d2 <- d %>% mutate(prog = as.factor(prog))
d2 <- d %>% mutate(prog = factor(prog, labels = prog_labels))
typeof(d2$prog)
is.factor(d2$prog)

# relationship between math score and num_awards (also considering the program)
d2 %>%
	ggplot(aes(x = math, y = num_awards, shape = prog, color = prog)) +
	geom_jitter(size = 4, width = 0.2, height = 0.2)

# relationship between work (the number of hours that student spent at work on average per week)
# and num_awards (also considering the program)
d2 %>%
	ggplot(aes(x = work, y = num_awards, shape = prog, color = prog)) +
	geom_jitter(size = 4, width = 0.2, height = 0.2)

# covariation and correlation matrices might be useful
d.cov <- cov(d)
d.cor <- cor(d)
```

## Simple linear model
Let us start with an ordinary linear model with no feature transformations. Explain how far the model works (does it meet formal assumptions?, does it overcome the null model?). Which predictors would you keep there and which of them are not useful? Use the standard evaluation procedures that we have for linear models.
```{r}
simple_lm <- lm(num_awards ~ ., d)
summary(simple_lm)

simple_lm_preds <- predict(simple_lm, d)

# What are the shortcomings of the simple regression?
# Among other things, it might predict a negative number of awards which makes no sense.
# But we could just consder all values below 0 as 0.
plot(
	x = d$num_awards + runif(nrow(d), -0.2, 0.2),
	y = simple_lm_preds,
	xlab = 'real no. awards',
	ylab = 'predicted no. awards',
	ylim = c(-1, 6)
)

# lm debug plots
par(mfrow = c(2, 2))
plot(simple_lm)

# Refer to the lectures what this call measures
anova(lm(num_awards ~ math + prog + work, d))
```
### Add your verbal summary here (1p):

When we fitted an ordinary linear model using `lm` we got the following model:

$$ num_awards = -2.536 + 0.110 * prog + 0.057 * math - 0.002 work $$

First, from the model summary, we can tell that **our fitted model is significantly better than the intercept-only model*** _(using F-test, based on the value of F-statistic, which is 22.16 on 3 and 196 DF, we reject the null hypothesis (p-value << 0.05) that the intercept-only model fits the data as well as our model)_.

Before interpreting the model, we should check whether the formal assumptions (noise normality, linearity, homoscedasticity) of OLS are met. Residual plots and the Q-Q plot are useful for this. From the "Residuals vs Fitted" plot, we can note the **homoscedasticity is most probably violated** (variance is not constant, i.e., we have heteroscedasticity): as we move to the right on the x-axis, the spread of the residuals seems to be increasing (and further, the spread is not symmetrical about the x-axis). Next, from the "Normal Q-Q" plot, we can tell that the noise **normality assumption is a bit violated**. All points should be on (or close to) the dashed line, but in our case, the points in the right part of the plot are shifted to the top. This particular shape expresses the fact that the data (their error dist.) are skewed on one side. These violations are no surprise since `num_awards` represents **count data**. Count data are characterized by heteroscedasticity and their error distribution is skewed.

Additionally, the model might also **predict a negative number of awards** which makes no sense.

To sum up, the simple linear regression (OLS) is not a good fit for our data, so we need to be careful when interpreting or trusting the fitted model.

Nevertheless, let's continue with evaluating our fitted model.

Adjusted R-squared is only 0.2418 which means our model explains only 24 % of the variance in the data.

Looking at the coefficients table, we can see that only the **math** predictor is significant.
By increasing a math score by one point, the number of awards increases by 0.057. This makes sense as students with higher math exam scores could be more likely to receive a higher number of awards.

Also, by looking at ANOVA table, we would say that only **math** predictor is worth keeping in our model.


## Generalized linear model
Now, the goal is to implement a generalized linear model that conceptually fits the given task. Do not transform the predictors yet, use them as they are, or omit them from the model. Once you obtain your model, interpret the effect of the *math* predictor on the outcome. How (according to your model) increasing a math score by one point affects the number of awards won?

Explain why the model overcomes the previous linear model, or justify that the generalized model is not needed. Compare the models theoretically as well as technically in terms of a proper quality measure. Note: The difference between the models cannot be statistically tested.
```{r}
# # Step 1:
# num_awards is a discrete outcome (aka reponse or depdenent) variable
# that represents count data. It is always non-negative.
# Let's assess the applicability of Poisson regression.
# Slice the data at each math-points-group and observe the distribution of the count response (num_awards).
bin_size <- 5 # math points
d %>%
	mutate(bin = round(math / bin_size) * bin_size) %>%
	ggplot(aes(x = num_awards)) +
	geom_histogram(binwidth = 0.5, center = 0) +
	facet_grid(cols = vars(bin)) +
	ggtitle("num_awards per math score group")

# # Step 2:
# Let's try the Poisson regression, which should be a better fit than OLS
# - we have count data and its distribution seems to be Poisson.
# Use only math predictor.
poiss_v1_glm <- glm(num_awards ~ math, family = "poisson", data = d)
print('poiss_v1_glm:')
summary(poiss_v1_glm)

# predict
poiss_v1_glm_preds <- predict(poiss_v1_glm, d, type = "response")
poiss_v1_glm_preds_r <- round(poiss_v1_glm_preds)

# vizualize
d %>%
	mutate(preds = poiss_v1_glm_preds) %>%
	ggplot(aes(x = math, y = num_awards)) +
	geom_point(size = 1.5, color = 'gray') +
	geom_jitter(size = 1.5, width = 0.3, height = 0.3, color = 'green') +
	# ggplot can do glm by itself
	geom_smooth(method = "glm", formula = y ~ x, method.args = list(family = "poisson")) +
	geom_smooth(method = "glm", formula = y ~ x, method.args = list(family = "gaussian")) +
	# but let's also vizualize the predictions manually
	geom_point(aes(x = math, y = poiss_v1_glm_preds, color = "Poiss predictions")) +
	theme_minimal()

# Step 3: Compare with the previous simple linear regression model
# We need some common ground for comparison,
# so let's fit the linear regression again, but this time with the glm() function
# (family = "gaussian" is equivalent to the simple linear regression).
simple_glm_full <- glm(num_awards ~ ., family = "gaussian", data = d)
print('simple_glm_full:')
summary(simple_glm_full)
simple_glm_math <- glm(num_awards ~ math, family = "gaussian", data = d)
print('simple_glm_math:')
summary(simple_glm_math)
```
### Add your verbal summary here (1p):

Our response variable (number of awards) represents count data. For that Poisson regression could be more appropriate.

First, we assess the applicability of Poisson regression by slicing the data at each math-points-group and observing the distribution (histogram) of the count response (num_awards). We conclude, that, indeed, for each level of x (math), the responses (num_awards) follows a Poisson distribution.

We fit the model using `glm(num_awards ~ math, family = "poisson", data = d)` (Poisson regression, using only math predictor).

According to our model, relationship between **math** and **num_wards** won is multiplicative.
By increasing a math score by one point, the *num_wards* won multiplies by $e^0.086$ where 0.086 is the math coefficient from the Poisson reg. coefficients table.
```{r}
num_awards_math_1 <- predict(poiss_v1_glm, newdata = data.frame(math = 45), type = "response")[[1]]
num_awards_math_2 <- predict(poiss_v1_glm, newdata = data.frame(math = 46), type = "response")[[1]]

num_awards_math_2_test <- num_awards_math_1 * exp(poiss_v1_glm$coefficients[["math"]])

# should be the same (diff 0):
print(paste("diff =", num_awards_math_2 - num_awards_math_2_test))
```

In order to compare Poisson model with the simple linear model, we need to find some common ground.
We fit simple linear model again, but this time using `glm(num_awards ~ math, family = "gaussian", data = d)`.
Then we can compare **AIC**. Our Poisson model has a lower (which is better) AIC (384.08) than the simple linear model (AIC 538.8).

Our Poisson model overcomes the simple linear model because Poisson regression is better suited for count data. Among other things, Poisson regression does not predict negative counts.


## Feature transformations and final model
*prog* and *work* did not prove to be effective predictors so far. Visualize these predictors as well as their relationship with the outcome variable. Based on the observations, propose suitable transformations for them (or, justify that they are truly uninformative for prediction of *num_awards*) and implement them into the best model found by now. Use the *compareGLM()* function to validate that your new GLMs indeed improved over the simple one.
```{r}
# Note:
#   We defined d2 with the prog column of type factor in Section "A few preliminary explorations",
#   where we also did a few visualizations some of which we will repeat below.

d2 %>%
	group_by(num_awards, prog) %>%
	summarise(count = n()) %>%
	ggplot(aes(x = num_awards, y = count, fill = prog)) +
	geom_col(position = "dodge") +
	xlab("Number of awards") +
	ylab("no. students")

d2 %>%
	ggplot(aes(x = work, y = num_awards, color = prog)) +
	geom_point() +
	xlab("Work hours avg per week") +
	ylab("Number of awards")

# relationship between math score and num_awards (also considering the program)
d2 %>%
	ggplot(aes(x = math, y = num_awards, shape = prog, color = prog)) +
	geom_jitter(size = 4, width = 0.2, height = 0.2) +
	xlab("Math final exam score") +
	ylab("Number of awards")

# relationship between work (the number of hours that student spent at work on average per week)
# and num_awards (also considering the program)
d2 %>%
	ggplot(aes(x = work, y = num_awards)) +
	geom_jitter(aes(shape = prog, color = prog), size = 4, width = 0.2, height = 0.2) +
	geom_smooth(method = "glm", formula = y ~ poly(x, 2), method.args = list(family = "poisson")) +
	xlab("Work hours avg per week") +
	ylab("Number of awards")

d3 <- d %>% mutate(
	prog2 = (abs(prog - 2L) * -1L) + 1L,
	prog2_f = factor(prog2, labels = c("0: Non-academic", "1: Academic")),
	prog_f = factor(prog, labels = prog_labels)
)

poiss_v2_glm <- glm(num_awards ~ math + poly(prog, 2) + poly(work, 2), family = "poisson", data = d)
print('poiss_v2_glm:')
summary(poiss_v2_glm)

# predict
poiss_v2_glm_preds <- predict(poiss_v2_glm, d, type = "response")
poiss_v2_glm_preds_r <- round(poiss_v2_glm_preds)

d3 %>%
	ggplot(aes(x = prog, y = num_awards)) +
	geom_smooth(method = "glm", formula = y ~ poly(x, 2), method.args = list(family = "poisson")) +
	geom_jitter(aes(color = prog_f))

d3 %>%
	ggplot(aes(x = prog2, y = num_awards)) +
	geom_smooth(method = "glm", formula = y ~ x, method.args = list(family = "poisson")) +
	geom_jitter(aes(color = prog2_f))


# TODO: math exam score by prog
d3 %>%
	ggplot(aes(x = math)) +
	geom_histogram(binwidth = 5, center = 0, fill = 'gray', color = 'white') +
	facet_grid(cols = vars(prog_f)) +
	ggtitle("math exam score per prog")


d3.cor <- d3 %>% select(-prog_f, -prog2_f) %>% cor()

# TODO: transform predictors



```

### Add your verbal summary here (2p):

TODO


## Ablation study through cross-validation
Recap all the models considered previously and evaluate them through cross-validation. You can start with the most simple null model and gradually add the previously discussed improvements. See their role in terms of MAE, RMSE and other commonly used criteria. The procedure outlined below proposes to work with *train* function from *caret* package, you can only add more models to evaluate and compare.
```{r,warning=FALSE}
train_control <- trainControl(method = "cv", number = 10)

# CV of the null lm model
print(train(
	x = data.frame(rep(1, nrow(d))),
	y = d$num_awards,
	method = "lm",
	trControl = train_control
)$results)

# CV of the simple linear model with all predictors
print('t' + train(
	x = d %>% select(prog, math, work),
	y = d$num_awards,
	method = "lm",
	trControl = train_control
)$results)

# CV of the simple linear model with math predictor only
summary(
	train(
		x = d %>% select(math),
		y = d$num_awards,
		trControl = train_control,
		method = "glm",
		family = "poisson",
	)
)

```
### Add your verbal summary here (1p):

TODO
